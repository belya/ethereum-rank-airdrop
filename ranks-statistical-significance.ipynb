{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, ShuffleSplit\n",
    "import networkx as nx\n",
    "\n",
    "import itertools\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.adjacency_list_to_graph import build_graph\n",
    "from common.calculate_spring_rank import calculate_spring_rank\n",
    "from common.graph_to_matrix import build_matrix\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical significance\n",
    "\n",
    "This page shows and explains the process of finding out SpringRank ranks statistical significance, that are extremely important to get really impressive results during genesis generation process (ahem, during the next few months)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. Building graph of transactions\n",
    "2. Inferring $\\beta$\n",
    "3. Inferring $c$\n",
    "4. Cross-validation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building graph\n",
    "Throughout the process of model calibration we'll use only part of Ethereum graph. It is placed in file named below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"./part_data\", sep=\" \", names=[\"from\", \"to\", \"value\", \"block\"])\n",
    "transactions_df[\"value\"] = 1\n",
    "transactions_df = transactions_df.sort_values(\"block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"./Cit-HepPh.txt\", names=[\"from\", \"to\"], sep=\"\\t\")\n",
    "transactions_df[\"value\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, addresses=None):\n",
    "    if addresses:\n",
    "        dataset = dataset[dataset[\"from\"].isin(addresses) & dataset[\"to\"].isin(addresses)]\n",
    "    return dataset.groupby([\"from\", \"to\"])[\"value\"].sum().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ranks(dataset, alpha):\n",
    "    edges = dataset[\"value\"].to_dict()\n",
    "    graph = build_graph(edges)\n",
    "    nodes = list(graph)\n",
    "    A = build_matrix(graph, nodes)\n",
    "    iterations, raw_rank = calculate_spring_rank(A, alpha=alpha)\n",
    "    \n",
    "    rank = pd.DataFrame()\n",
    "    rank[\"address\"] = nodes\n",
    "    rank[\"rank\"] = raw_rank\n",
    "    \n",
    "    return rank.set_index(\"address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"/device:XLA_CPU:0\"\n",
       "device_type: \"XLA_CPU\"\n",
       "memory_limit: 17179869184\n",
       "locality {\n",
       "}\n",
       "incarnation: 5419152033787157668\n",
       "physical_device_desc: \"device: XLA_CPU device\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nessesary_tf_elements(edges, ranks):\n",
    "    from_indices = tf.transpose(tf.transpose(edges)[0])\n",
    "    to_indices = tf.transpose(tf.transpose(edges)[1])\n",
    "    edges_values =  tf.transpose(tf.transpose(edges)[2])\n",
    "\n",
    "    nodes, reindexed_from_indices = tf.unique(from_indices)\n",
    "    reindexed_edges = tf.transpose(tf.stack([reindexed_from_indices, to_indices]))\n",
    "\n",
    "    s = tf.expand_dims(ranks, 1)\n",
    "    chunk_s = tf.gather_nd(tf.expand_dims(ranks, 1), tf.expand_dims(nodes, 1))\n",
    "\n",
    "    graph_matrix_shape = [tf.reduce_max(reindexed_from_indices) + 1, tf.shape(ranks)[0]]\n",
    "    A = tf.sparse_to_dense(sparse_indices=reindexed_edges, sparse_values=edges_values, output_shape=graph_matrix_shape, validate_indices=False)\n",
    "    A = tf.cast(A, tf.float32)\n",
    "    \n",
    "    D = tf.math.sqrt(tf.math.squared_difference(chunk_s, tf.transpose(s)))\n",
    "\n",
    "    return A, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making ground state energy significance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_indices = {node: index for index, node in enumerate(np.unique(transactions_df[[\"from\", \"to\"]].values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_dataset(dataset, probability):\n",
    "    dataset[\"random\"] = np.random.rand(dataset.shape[0]) > probability\n",
    "    new_from = dataset[\"from\"] * dataset[\"random\"] + dataset[\"to\"] * (1 - dataset[\"random\"])\n",
    "    new_to = dataset[\"from\"] * (1 - dataset[\"random\"]) + dataset[\"to\"] * dataset[\"random\"]\n",
    "    dataset[\"from\"] = new_from\n",
    "    dataset[\"to\"] = new_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "all_edges = tf.placeholder(tf.int32, shape=(None, 3))\n",
    "edges_dataset = tf.data.Dataset.from_tensor_slices(all_edges).batch(BATCH_SIZE)\n",
    "edges_iterator = edges_dataset.make_initializable_iterator()\n",
    "edges_chunk = edges_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = tf.placeholder(tf.float32, shape=(None, ))\n",
    "\n",
    "A, D = get_nessesary_tf_elements(edges_chunk, ranks)\n",
    "D = (D - 1) * (D - 1)\n",
    "\n",
    "H_matrix = A * D\n",
    "H = 0.5 * tf.reduce_sum(H_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_H(edges_feed, ranks_feed):\n",
    "    global H\n",
    "    total_H = 0\n",
    "    with tf.device(\"/device:XLA_CPU:0\"):\n",
    "        sess.run(edges_iterator.initializer, {all_edges: edges_feed})\n",
    "        while True:\n",
    "            try:\n",
    "                total_H += sess.run(H, {ranks: ranks_feed})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        return total_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сам тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_transactions_df = transactions_df.copy()\n",
    "prepared_transactions_df[\"from\"] = transactions_df[\"from\"].apply(lambda x: nodes_indices[x])\n",
    "prepared_transactions_df[\"to\"] = transactions_df[\"to\"].apply(lambda x: nodes_indices[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51593.92010498047"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_H(state_values, state_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf851b157536472eb865a52a19a98b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph contains 421578 edges for 34546 nodes\n",
      "Estimated size of A is 5.2 MB RAM\n",
      "Matrix A takes 5.2 MB RAM\n",
      "Matrix has 3.53e-04 density\n",
      "01:03:18.032305 Calculating Anj ....\n",
      "01:03:18.435376 Calculating Ajn ....\n",
      "01:03:18.638905 Calculating A_o ....\n",
      "01:03:18.657155 Calculating B ....\n",
      "01:03:18.672097 Matrix B takes 14.8 MB RAM\n",
      "01:03:18.672184 Calculating b ....\n",
      "01:03:18.674923 Solving Bx=b equation using 'bicgstab' iterative method\n",
      "135981.4189453125 135981.4189453125 135981.4189453125 135981.4189453125 135981.4189453125\n"
     ]
    }
   ],
   "source": [
    "energies = []\n",
    "for i in tqdm_notebook(range(0, 1)):\n",
    "    state = create_dataset(prepared_transactions_df)\n",
    "    state_values = state.reset_index().values\n",
    "    state_ranks = find_ranks(state, alpha=0).sort_values(\"address\")[\"rank\"].values\n",
    "    energies += [calculate_H(state_values, state_ranks)]\n",
    "    randomize_dataset(prepared_transactions_df, 0.5)\n",
    "    print(energies[0], np.percentile(energies, 20), np.percentile(energies, 50), np.percentile(energies, 70), np.percentile(energies, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29%\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:.0%} percents from \".format(energies[0] / np.percentile(energies, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring $\\beta$\n",
    "\n",
    "Except the calculated ranks, the described model has two parameters - temperature $\\beta$ and density $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find $\\beta$, we'll treat ranks as constant values and apply maximum likelihood procedure described below:\n",
    "\n",
    "$$L(A|s, \\beta) = -\\beta H(s) - M \\log\\sum_{i, j}\\exp -\\frac{\\beta}{2}(s_i - s_j - 1)^2$$\n",
    "\n",
    "As a result, we'll get $\\beta$ parameter that minimizes loss function $L$ in the presence of fixed ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_indices = {node: index for index, node in enumerate(np.unique(transactions_df[[\"from\", \"to\"]].values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_transactions_df = transactions_df.copy()\n",
    "prepared_transactions_df[\"from\"] = transactions_df[\"from\"].apply(lambda x: nodes_indices[x])\n",
    "prepared_transactions_df[\"to\"] = transactions_df[\"to\"].apply(lambda x: nodes_indices[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(prepared_transactions_df)\n",
    "all_edges_feed = train_dataset.reset_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph contains 421578 edges for 34546 nodes\n",
      "Estimated size of A is 5.2 MB RAM\n",
      "Matrix A takes 5.2 MB RAM\n",
      "Matrix has 3.53e-04 density\n",
      "01:05:21.653233 Calculating Anj ....\n",
      "01:05:21.881943 Calculating Ajn ....\n",
      "01:05:22.085816 Calculating A_o ....\n",
      "01:05:22.103507 Calculating B ....\n",
      "01:05:22.117827 Matrix B takes 14.8 MB RAM\n",
      "01:05:22.117899 Calculating b ....\n",
      "01:05:22.120384 Solving Bx=b equation using 'bicgstab' iterative method\n"
     ]
    }
   ],
   "source": [
    "ranks_feed = find_ranks(train_dataset, alpha=0).sort_values(\"address\")[\"rank\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "SHUFFLE_SIZE = 20000\n",
    "all_edges = tf.placeholder(tf.int32, shape=(None, 3))\n",
    "edges_dataset = tf.data.Dataset.from_tensor_slices(all_edges).shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE).repeat()\n",
    "edges_iterator = edges_dataset.make_initializable_iterator()\n",
    "edges_chunk = edges_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-size chunks\n",
    "\n",
    "ranks = tf.placeholder(tf.float32, shape=(None, ))\n",
    "beta = tf.Variable(0.0)\n",
    "\n",
    "A, D = get_nessesary_tf_elements(edges_chunk, ranks)\n",
    "D = (D - 1) * (D - 1)\n",
    "\n",
    "log_D = tf.log(tf.reduce_sum(tf.exp(- beta / 2 * D)))\n",
    "\n",
    "H_matrix = A * D\n",
    "H = 0.5 * tf.reduce_sum(H_matrix)\n",
    "\n",
    "M = tf.reduce_sum(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Batched chunks\n",
    "# ranks = tf.placeholder(tf.float32, shape=(None, ))\n",
    "# edges = edges_chunk\n",
    "# beta = tf.Variable(0.0)\n",
    "\n",
    "# edges_indices = tf.transpose(tf.transpose(edges)[0:2])\n",
    "# nodes, reindexed_edges = tf.unique(tf.reshape(edges_indices, shape=(-1, )))\n",
    "# s = tf.gather_nd(tf.expand_dims(ranks, 1), tf.expand_dims(nodes, 1))\n",
    "# reindexed_edges = tf.reshape(reindexed_edges, shape=(-1, 2))\n",
    "# edges_values =  tf.transpose(tf.transpose(edges)[2])\n",
    "# graph_matrix_shape = [tf.reduce_max(reindexed_edges) + 1] * 2\n",
    "# A = tf.sparse_to_dense(sparse_indices=reindexed_edges, sparse_values=edges_values, output_shape=graph_matrix_shape, validate_indices=False)\n",
    "# A = tf.cast(A, tf.float32)\n",
    "\n",
    "# D = tf.math.squared_difference(s, tf.transpose(s) + 1)\n",
    "# log_D = tf.log(tf.reduce_sum(tf.exp(- beta / 2 * D)))\n",
    "\n",
    "# H_matrix = A * D\n",
    "# H = 0.5 * tf.reduce_sum(H_matrix)\n",
    "\n",
    "# M = tf.reduce_sum(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = - beta * H - M * log_D\n",
    "optimizer = tf.train.AdamOptimizer(0.1).minimize(-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(edges_iterator.initializer, {all_edges: all_edges_feed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44351d821504010b35d44692a62971e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Loss: -180908.16\n",
      "Beta: 0.1\n",
      "Iteration: 1\n",
      "Loss: -183836.28\n",
      "Beta: 0.19991183\n",
      "Iteration: 2\n",
      "Loss: -184312.25\n",
      "Beta: 0.29943252\n",
      "Iteration: 3\n",
      "Loss: -184177.34\n",
      "Beta: 0.39821112\n",
      "Iteration: 4\n",
      "Loss: -184178.52\n",
      "Beta: 0.4965881\n",
      "Iteration: 5\n",
      "Loss: -183900.28\n",
      "Beta: 0.59509087\n",
      "Iteration: 6\n",
      "Loss: -183637.25\n",
      "Beta: 0.69311565\n",
      "Iteration: 7\n",
      "Loss: -183495.4\n",
      "Beta: 0.79018533\n",
      "Iteration: 8\n",
      "Loss: -183625.84\n",
      "Beta: 0.8858615\n",
      "Iteration: 9\n",
      "Loss: -183422.98\n",
      "Beta: 0.9793561\n",
      "Iteration: 10\n",
      "Loss: -188263.6\n",
      "Beta: 1.0662162\n",
      "Iteration: 11\n",
      "Loss: -188402.0\n",
      "Beta: 1.1441854\n",
      "Iteration: 12\n",
      "Loss: -187897.98\n",
      "Beta: 1.2131641\n",
      "Iteration: 13\n",
      "Loss: -187066.0\n",
      "Beta: 1.2758367\n",
      "Iteration: 14\n",
      "Loss: -186542.02\n",
      "Beta: 1.3347199\n",
      "Iteration: 15\n",
      "Loss: -186311.97\n",
      "Beta: 1.3913959\n",
      "Iteration: 16\n",
      "Loss: -186065.9\n",
      "Beta: 1.4453229\n",
      "Iteration: 17\n",
      "Loss: -185604.22\n",
      "Beta: 1.4997649\n",
      "Iteration: 18\n",
      "Loss: -185114.67\n",
      "Beta: 1.5544871\n",
      "Iteration: 19\n",
      "Loss: -184689.22\n",
      "Beta: 1.6103445\n",
      "Iteration: 20\n",
      "Loss: -183756.52\n",
      "Beta: 1.667981\n",
      "Iteration: 21\n",
      "Loss: -177442.22\n",
      "Beta: 1.7213459\n",
      "Iteration: 22\n",
      "Loss: -181880.73\n",
      "Beta: 1.7779083\n",
      "Iteration: 23\n",
      "Loss: -183460.08\n",
      "Beta: 1.8374628\n",
      "Iteration: 24\n",
      "Loss: -183808.94\n",
      "Beta: 1.8988808\n",
      "Iteration: 25\n",
      "Loss: -183568.8\n",
      "Beta: 1.9612024\n",
      "Iteration: 26\n",
      "Loss: -183423.77\n",
      "Beta: 2.0242636\n",
      "Iteration: 27\n",
      "Loss: -183149.28\n",
      "Beta: 2.0879042\n",
      "Iteration: 28\n",
      "Loss: -183196.7\n",
      "Beta: 2.1511405\n",
      "Iteration: 29\n",
      "Loss: -183213.7\n",
      "Beta: 2.2127383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-8c3e0a497830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mranks_feed\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(0, 500)):\n",
    "    feed_dict = {ranks: ranks_feed}\n",
    "    sess.run(optimizer, feed_dict)\n",
    "    print(\"Iteration:\", i)\n",
    "    print(\"Loss:\", sess.run(loss, feed_dict))\n",
    "    print(\"Beta:\", sess.run(beta, feed_dict))\n",
    "#     print(\"Energy:\", sess.run(beta * H, feed_dict))\n",
    "#     print(\"Sum:\", sess.run(M * log_D, feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring $c$\n",
    "We'll infer generative model density simply by dividing the sum of predicted edges by the sum of edges that are presented in train dataset.\n",
    "\n",
    "SpringRank generative model includes formulas to find $E_{ij}$ - number of edges between $i$ and $j$ vertices in both directions, and $P_{ij}$ - proportion of edges that are going from vertex $i$ to vertex $j$. These formulas are going below:\n",
    "\n",
    "$$P_{ij} = \\frac{1}{1 + e^{-2\\beta(s_i - s_j)}}, P_{ji} = 1 - P_{ij}$$\n",
    "\n",
    "$$E_{ij} = c \\exp{-\\frac{\\beta}{2}(s_i - s_j - 1)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "all_edges = tf.placeholder(tf.int32, shape=(None, 3))\n",
    "edges_dataset = tf.data.Dataset.from_tensor_slices(all_edges).batch(BATCH_SIZE)\n",
    "edges_iterator = edges_dataset.make_initializable_iterator()\n",
    "edges_chunk = edges_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = tf.placeholder(tf.float32, shape=(None, ))\n",
    "beta = tf.placeholder(tf.float32)\n",
    "\n",
    "A, D = get_nessesary_tf_elements(edges_chunk, ranks)\n",
    "D_squared = (D - 1) * (D - 1)\n",
    "\n",
    "M = tf.reduce_sum(A)\n",
    "E = tf.exp(-beta / 2 * D_squared)\n",
    "P = 1 / (1 + tf.exp(-2 * beta * D))\n",
    "predicted_M = tf.reduce_sum(E * P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_density(edges_feed, ranks_feed, beta_feed):\n",
    "    total_M = 0\n",
    "    total_predicted_M = 0\n",
    "    sess.run(edges_iterator.initializer, {all_edges: edges_feed})\n",
    "    while True:\n",
    "        try:\n",
    "            partial_M, partial_predicted_M = sess.run([M, predicted_M], {ranks: ranks_feed, beta: beta_feed})\n",
    "            total_M += partial_M\n",
    "            total_predicted_M += partial_predicted_M\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    return total_M, total_predicted_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_M, total_predicted_M = infer_density(all_edges_feed, ranks_feed, 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006399440370701949"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_M / total_predicted_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get P and E predictions\n",
    "- Get loss and accuracy for a batch\n",
    "$$\\sigma_a = 1 - \\frac{1}{M + M_{pred}}\\sum_{i,j}|{A_{ij} - E_{ij}P_{ij}}|$$\n",
    "- Get test accuracy / loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "all_edges = tf.placeholder(tf.int32, shape=(None, 3))\n",
    "edges_dataset = tf.data.Dataset.from_tensor_slices(all_edges).batch(BATCH_SIZE)\n",
    "edges_iterator = edges_dataset.make_initializable_iterator()\n",
    "edges_chunk = edges_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = tf.placeholder(tf.float32, shape=(None, ))\n",
    "beta = tf.placeholder(tf.float32)\n",
    "c = tf.placeholder(tf.float32)\n",
    "\n",
    "A, D = get_nessesary_tf_elements(edges_chunk, ranks)\n",
    "D_squared = (D - 1) * (D - 1)\n",
    "\n",
    "M = tf.reduce_sum(A)\n",
    "\n",
    "E = tf.exp(-beta / 2 * D_squared)\n",
    "P = 1 / (1 + tf.exp(-2 * beta * D))\n",
    "predicted_M = tf.reduce_sum(E * P)\n",
    "\n",
    "accuracy = 1 - 1 / (predicted_M +  M) * tf.reduce_sum(tf.math.abs(A - E * P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(edges_feed, ranks_feed, beta_feed, c_feed):\n",
    "    total_accuracy = []\n",
    "    sess.run(edges_iterator.initializer, {all_edges: edges_feed})\n",
    "    while True:\n",
    "        try:\n",
    "            total_accuracy += [sess.run(accuracy, {ranks: ranks_feed, beta: beta_feed, c: c_feed})]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = ShuffleSplit(n_splits=5)\n",
    "\n",
    "train_index, test_index = next(split.split(transactions_df))\n",
    "train_df = transactions_df.loc[train_index].copy()\n",
    "test_df = transactions_df.loc[test_index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_indices = {node: index for index, node in enumerate(np.unique(train_df[[\"from\", \"to\"]].values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"from\"] = train_df[\"from\"].apply(lambda x: nodes_indices[x])\n",
    "train_df[\"to\"] = train_df[\"to\"].apply(lambda x: nodes_indices[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"from\"] = test_df[\"from\"].apply(lambda x: nodes_indices.get(x, np.nan))\n",
    "test_df[\"to\"] = test_df[\"to\"].apply(lambda x: nodes_indices.get(x, np.nan))\n",
    "test_df = test_df[~np.isnan(test_df[\"to\"]) & ~np.isnan(test_df[\"from\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_dataset(train_df)\n",
    "test = create_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph contains 379420 edges for 34408 nodes\n",
      "Estimated size of A is 4.7 MB RAM\n",
      "Matrix A takes 4.7 MB RAM\n",
      "Matrix has 3.20e-04 density\n",
      "02:47:16.567258 Calculating Anj ....\n",
      "02:47:16.818959 Calculating Ajn ....\n",
      "02:47:17.053769 Calculating A_o ....\n",
      "02:47:17.073155 Calculating B ....\n",
      "02:47:17.086724 Matrix B takes 13.8 MB RAM\n",
      "02:47:17.086809 Calculating b ....\n",
      "02:47:17.089451 Solving Bx=b equation using 'bicgstab' iterative method\n"
     ]
    }
   ],
   "source": [
    "train_state_values = train.reset_index().values\n",
    "train_state_ranks = find_ranks(train, alpha=0).sort_values(\"address\")[\"rank\"].values\n",
    "\n",
    "test_state_values = test.reset_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_feed = 3.5\n",
    "c_feed = 1 / 1562.636640194697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0010347366, 0.0011512041, 0.0010307431, 0.0010216832, 0.0009849072, 0.0010280609, 0.0011312962, 0.0011048317, 0.0011321902, 0.0011932254, 0.0011166334, 0.001090467, 0.0012654066, 0.0012270808, 0.0011395812, 0.001244545, 0.0010846853, 0.001150012, 0.0011752248, 0.00049483776, 0.00024801493, 0.0003848076, 0.0005041957, 0.00048965216, 0.00062191486, 0.00062310696, 0.00068348646, 0.00070917606, 0.00077968836, 0.00070369244, 0.0006920099, 0.0007894635, 0.0008816719, 0.0008684397, 0.0010074377, 0.0008981824, 0.0009996891, 0.0010525584]\n"
     ]
    }
   ],
   "source": [
    "print(get_accuracy(train_state_values, train_state_ranks, beta_feed, c_feed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00017493963, 0.00017940998, 0.00011265278, 0.00014275312, 0.00016230345]\n"
     ]
    }
   ],
   "source": [
    "print(get_accuracy(test_state_values, train_state_ranks, beta_feed, c_feed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
