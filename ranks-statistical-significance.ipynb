{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, ShuffleSplit\n",
    "import networkx as nx\n",
    "\n",
    "import itertools\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.adjacency_list_to_graph import build_graph\n",
    "from common.calculate_spring_rank import calculate_spring_rank\n",
    "from common.graph_to_matrix import build_matrix\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical significance\n",
    "\n",
    "This page shows and explains the process of finding out SpringRank ranks statistical significance, that are extremely important to get really impressive results during genesis generation process (ahem, during the next few months)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. Building graph of transactions\n",
    "2. Inferring $\\beta$\n",
    "3. Inferring $c$\n",
    "4. Cross-validation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building graph\n",
    "Throughout the process of model calibration we'll use only part of Ethereum graph. It is placed in file named below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"./part_data\", sep=\" \", names=[\"from\", \"to\", \"value\", \"block\"])\n",
    "transactions_df[\"value\"] = 1\n",
    "transactions_df = transactions_df.sort_values(\"block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"./Cit-HepPh.txt\", names=[\"from\", \"to\"], sep=\"\\t\")\n",
    "transactions_df[\"value\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, addresses=None):\n",
    "    if addresses:\n",
    "        dataset = dataset[dataset[\"from\"].isin(addresses) & dataset[\"to\"].isin(addresses)]\n",
    "    return dataset.groupby([\"from\", \"to\"])[\"value\"].sum().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ranks(dataset, alpha):\n",
    "    edges = dataset[\"value\"].to_dict()\n",
    "    graph = build_graph(edges)\n",
    "    nodes = list(graph)\n",
    "    A = build_matrix(graph, nodes)\n",
    "    iterations, raw_rank = calculate_spring_rank(A, alpha=alpha)\n",
    "    \n",
    "    rank = pd.DataFrame()\n",
    "    rank[\"address\"] = nodes\n",
    "    rank[\"rank\"] = raw_rank\n",
    "    \n",
    "    return rank.set_index(\"address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"/device:XLA_CPU:0\"\n",
       "device_type: \"XLA_CPU\"\n",
       "memory_limit: 17179869184\n",
       "locality {\n",
       "}\n",
       "incarnation: 5419152033787157668\n",
       "physical_device_desc: \"device: XLA_CPU device\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nessesary_tf_elements(edges, ranks):\n",
    "    from_indices = tf.transpose(tf.transpose(edges)[0])\n",
    "    to_indices = tf.transpose(tf.transpose(edges)[1])\n",
    "    edges_values =  tf.transpose(tf.transpose(edges)[2])\n",
    "\n",
    "    nodes, reindexed_from_indices = tf.unique(from_indices)\n",
    "    reindexed_edges = tf.transpose(tf.stack([reindexed_from_indices, to_indices]))\n",
    "\n",
    "    s = tf.expand_dims(ranks, 1)\n",
    "    chunk_s = tf.gather_nd(tf.expand_dims(ranks, 1), tf.expand_dims(nodes, 1))\n",
    "\n",
    "    graph_matrix_shape = [tf.reduce_max(reindexed_from_indices) + 1, tf.shape(ranks)[0]]\n",
    "    A = tf.sparse_to_dense(sparse_indices=reindexed_edges, sparse_values=edges_values, output_shape=graph_matrix_shape, validate_indices=False)\n",
    "    A = tf.cast(A, tf.float32)\n",
    "    \n",
    "    D = tf.math.sqrt(tf.math.squared_difference(chunk_s, tf.transpose(s)))\n",
    "\n",
    "    return A, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making ground state energy significance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_indices = {node: index for index, node in enumerate(np.unique(transactions_df[[\"from\", \"to\"]].values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_dataset(dataset, probability):\n",
    "    dataset[\"random\"] = np.random.rand(dataset.shape[0]) > probability\n",
    "    new_from = dataset[\"from\"] * dataset[\"random\"] + dataset[\"to\"] * (1 - dataset[\"random\"])\n",
    "    new_to = dataset[\"from\"] * (1 - dataset[\"random\"]) + dataset[\"to\"] * dataset[\"random\"]\n",
    "    dataset[\"from\"] = new_from\n",
    "    dataset[\"to\"] = new_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "all_edges = tf.placeholder(tf.int32, shape=(None, 3))\n",
    "edges_dataset = tf.data.Dataset.from_tensor_slices(all_edges).batch(BATCH_SIZE)\n",
    "edges_iterator = edges_dataset.make_initializable_iterator()\n",
    "edges_chunk = edges_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = tf.placeholder(tf.float32, shape=(None, ))\n",
    "\n",
    "A, D = get_nessesary_tf_elements(edges_chunk, ranks)\n",
    "D = (D - 1) * (D - 1)\n",
    "\n",
    "H_matrix = A * D\n",
    "H = 0.5 * tf.reduce_sum(H_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_H(edges_feed, ranks_feed):\n",
    "    global H\n",
    "    total_H = 0\n",
    "    with tf.device(\"/device:XLA_CPU:0\"):\n",
    "        sess.run(edges_iterator.initializer, {all_edges: edges_feed})\n",
    "        while True:\n",
    "            try:\n",
    "                total_H += sess.run(H, {ranks: ranks_feed})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        return total_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сам тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_transactions_df = transactions_df.copy()\n",
    "prepared_transactions_df[\"from\"] = transactions_df[\"from\"].apply(lambda x: nodes_indices[x])\n",
    "prepared_transactions_df[\"to\"] = transactions_df[\"to\"].apply(lambda x: nodes_indices[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51593.92010498047"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_H(state_values, state_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf851b157536472eb865a52a19a98b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph contains 421578 edges for 34546 nodes\n",
      "Estimated size of A is 5.2 MB RAM\n",
      "Matrix A takes 5.2 MB RAM\n",
      "Matrix has 3.53e-04 density\n",
      "01:03:18.032305 Calculating Anj ....\n",
      "01:03:18.435376 Calculating Ajn ....\n",
      "01:03:18.638905 Calculating A_o ....\n",
      "01:03:18.657155 Calculating B ....\n",
      "01:03:18.672097 Matrix B takes 14.8 MB RAM\n",
      "01:03:18.672184 Calculating b ....\n",
      "01:03:18.674923 Solving Bx=b equation using 'bicgstab' iterative method\n",
      "135981.4189453125 135981.4189453125 135981.4189453125 135981.4189453125 135981.4189453125\n"
     ]
    }
   ],
   "source": [
    "energies = []\n",
    "for i in tqdm_notebook(range(0, 1)):\n",
    "    state = create_dataset(prepared_transactions_df)\n",
    "    state_values = state.reset_index().values\n",
    "    state_ranks = find_ranks(state, alpha=0).sort_values(\"address\")[\"rank\"].values\n",
    "    energies += [calculate_H(state_values, state_ranks)]\n",
    "    randomize_dataset(prepared_transactions_df, 0.5)\n",
    "    print(energies[0], np.percentile(energies, 20), np.percentile(energies, 50), np.percentile(energies, 70), np.percentile(energies, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29%\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:.0%} percents from \".format(energies[0] / np.percentile(energies, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring $\\beta$\n",
    "\n",
    "Except the calculated ranks, the described model has two parameters - temperature $\\beta$ and density $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find $\\beta$, we'll treat ranks as constant values and apply maximum likelihood procedure described below:\n",
    "\n",
    "$$L(A|s, \\beta) = -\\beta H(s) - M \\log\\sum_{i, j}\\exp -\\frac{\\beta}{2}(s_i - s_j - 1)^2$$\n",
    "\n",
    "As a result, we'll get $\\beta$ parameter that minimizes loss function $L$ in the presence of fixed ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_indices = {node: index for index, node in enumerate(np.unique(transactions_df[[\"from\", \"to\"]].values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_transactions_df = transactions_df.copy()\n",
    "prepared_transactions_df[\"from\"] = transactions_df[\"from\"].apply(lambda x: nodes_indices[x])\n",
    "prepared_transactions_df[\"to\"] = transactions_df[\"to\"].apply(lambda x: nodes_indices[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(prepared_transactions_df)\n",
    "all_edges_feed = train_dataset.reset_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph contains 421578 edges for 34546 nodes\n",
      "Estimated size of A is 5.2 MB RAM\n",
      "Matrix A takes 5.2 MB RAM\n",
      "Matrix has 3.53e-04 density\n",
      "01:05:21.653233 Calculating Anj ....\n",
      "01:05:21.881943 Calculating Ajn ....\n",
      "01:05:22.085816 Calculating A_o ....\n",
      "01:05:22.103507 Calculating B ....\n",
      "01:05:22.117827 Matrix B takes 14.8 MB RAM\n",
      "01:05:22.117899 Calculating b ....\n",
      "01:05:22.120384 Solving Bx=b equation using 'bicgstab' iterative method\n"
     ]
    }
   ],
   "source": [
    "ranks_feed = find_ranks(train_dataset, alpha=0).sort_values(\"address\")[\"rank\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "SHUFFLE_SIZE = 20000\n",
    "all_edges = tf.placeholder(tf.int32, shape=(None, 3))\n",
    "edges_dataset = tf.data.Dataset.from_tensor_slices(all_edges).shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE).repeat()\n",
    "edges_iterator = edges_dataset.make_initializable_iterator()\n",
    "edges_chunk = edges_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-size chunks\n",
    "\n",
    "ranks = tf.placeholder(tf.float32, shape=(None, ))\n",
    "beta = tf.Variable(0.0)\n",
    "\n",
    "A, D = get_nessesary_tf_elements(edges_chunk, ranks)\n",
    "D = (D - 1) * (D - 1)\n",
    "\n",
    "log_D = tf.log(tf.reduce_sum(tf.exp(- beta / 2 * D)))\n",
    "\n",
    "H_matrix = A * D\n",
    "H = 0.5 * tf.reduce_sum(H_matrix)\n",
    "\n",
    "M = tf.reduce_sum(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Batched chunks\n",
    "# ranks = tf.placeholder(tf.float32, shape=(None, ))\n",
    "# edges = edges_chunk\n",
    "# beta = tf.Variable(0.0)\n",
    "\n",
    "# edges_indices = tf.transpose(tf.transpose(edges)[0:2])\n",
    "# nodes, reindexed_edges = tf.unique(tf.reshape(edges_indices, shape=(-1, )))\n",
    "# s = tf.gather_nd(tf.expand_dims(ranks, 1), tf.expand_dims(nodes, 1))\n",
    "# reindexed_edges = tf.reshape(reindexed_edges, shape=(-1, 2))\n",
    "# edges_values =  tf.transpose(tf.transpose(edges)[2])\n",
    "# graph_matrix_shape = [tf.reduce_max(reindexed_edges) + 1] * 2\n",
    "# A = tf.sparse_to_dense(sparse_indices=reindexed_edges, sparse_values=edges_values, output_shape=graph_matrix_shape, validate_indices=False)\n",
    "# A = tf.cast(A, tf.float32)\n",
    "\n",
    "# D = tf.math.squared_difference(s, tf.transpose(s) + 1)\n",
    "# log_D = tf.log(tf.reduce_sum(tf.exp(- beta / 2 * D)))\n",
    "\n",
    "# H_matrix = A * D\n",
    "# H = 0.5 * tf.reduce_sum(H_matrix)\n",
    "\n",
    "# M = tf.reduce_sum(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = - beta * H - M * log_D\n",
    "optimizer = tf.train.AdamOptimizer(0.1).minimize(-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(edges_iterator.initializer, {all_edges: all_edges_feed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44351d821504010b35d44692a62971e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Loss: -180908.16\n",
      "Beta: 0.1\n",
      "Iteration: 1\n",
      "Loss: -183836.28\n",
      "Beta: 0.19991183\n",
      "Iteration: 2\n",
      "Loss: -184312.25\n",
      "Beta: 0.29943252\n",
      "Iteration: 3\n",
      "Loss: -184177.34\n",
      "Beta: 0.39821112\n",
      "Iteration: 4\n",
      "Loss: -184178.52\n",
      "Beta: 0.4965881\n",
      "Iteration: 5\n",
      "Loss: -183900.28\n",
      "Beta: 0.59509087\n",
      "Iteration: 6\n",
      "Loss: -183637.25\n",
      "Beta: 0.69311565\n",
      "Iteration: 7\n",
      "Loss: -183495.4\n",
      "Beta: 0.79018533\n",
      "Iteration: 8\n",
      "Loss: -183625.84\n",
      "Beta: 0.8858615\n",
      "Iteration: 9\n",
      "Loss: -183422.98\n",
      "Beta: 0.9793561\n",
      "Iteration: 10\n",
      "Loss: -188263.6\n",
      "Beta: 1.0662162\n",
      "Iteration: 11\n",
      "Loss: -188402.0\n",
      "Beta: 1.1441854\n",
      "Iteration: 12\n",
      "Loss: -187897.98\n",
      "Beta: 1.2131641\n",
      "Iteration: 13\n",
      "Loss: -187066.0\n",
      "Beta: 1.2758367\n",
      "Iteration: 14\n",
      "Loss: -186542.02\n",
      "Beta: 1.3347199\n",
      "Iteration: 15\n",
      "Loss: -186311.97\n",
      "Beta: 1.3913959\n",
      "Iteration: 16\n",
      "Loss: -186065.9\n",
      "Beta: 1.4453229\n",
      "Iteration: 17\n",
      "Loss: -185604.22\n",
      "Beta: 1.4997649\n",
      "Iteration: 18\n",
      "Loss: -185114.67\n",
      "Beta: 1.5544871\n",
      "Iteration: 19\n",
      "Loss: -184689.22\n",
      "Beta: 1.6103445\n",
      "Iteration: 20\n",
      "Loss: -183756.52\n",
      "Beta: 1.667981\n",
      "Iteration: 21\n",
      "Loss: -177442.22\n",
      "Beta: 1.7213459\n",
      "Iteration: 22\n",
      "Loss: -181880.73\n",
      "Beta: 1.7779083\n",
      "Iteration: 23\n",
      "Loss: -183460.08\n",
      "Beta: 1.8374628\n",
      "Iteration: 24\n",
      "Loss: -183808.94\n",
      "Beta: 1.8988808\n",
      "Iteration: 25\n",
      "Loss: -183568.8\n",
      "Beta: 1.9612024\n",
      "Iteration: 26\n",
      "Loss: -183423.77\n",
      "Beta: 2.0242636\n",
      "Iteration: 27\n",
      "Loss: -183149.28\n",
      "Beta: 2.0879042\n",
      "Iteration: 28\n",
      "Loss: -183196.7\n",
      "Beta: 2.1511405\n",
      "Iteration: 29\n",
      "Loss: -183213.7\n",
      "Beta: 2.2127383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-8c3e0a497830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mranks_feed\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(0, 500)):\n",
    "    feed_dict = {ranks: ranks_feed}\n",
    "    sess.run(optimizer, feed_dict)\n",
    "    print(\"Iteration:\", i)\n",
    "    print(\"Loss:\", sess.run(loss, feed_dict))\n",
    "    print(\"Beta:\", sess.run(beta, feed_dict))\n",
    "#     print(\"Energy:\", sess.run(beta * H, feed_dict))\n",
    "#     print(\"Sum:\", sess.run(M * log_D, feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring $c$\n",
    "We'll infer generative model density simply by dividing the sum of predicted edges by the sum of edges that are presented in train dataset.\n",
    "\n",
    "SpringRank generative model includes formulas to find $E_{ij}$ - number of edges between $i$ and $j$ vertices in both directions, and $P_{ij}$ - proportion of edges that are going from vertex $i$ to vertex $j$. These formulas are going below:\n",
    "\n",
    "$$P_{ij} = \\frac{1}{1 + e^{-2\\beta(s_i - s_j)}}, P_{ji} = 1 - P_{ij}$$\n",
    "\n",
    "$$E_{ij} = c \\exp{-\\frac{\\beta}{2}(s_i - s_j - 1)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "all_edges = tf.placeholder(tf.int32, shape=(None, 3))\n",
    "edges_dataset = tf.data.Dataset.from_tensor_slices(all_edges).batch(BATCH_SIZE)\n",
    "edges_iterator = edges_dataset.make_initializable_iterator()\n",
    "edges_chunk = edges_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = tf.placeholder(tf.float32, shape=(None, ))\n",
    "beta = tf.placeholder(tf.float32)\n",
    "\n",
    "A, D = get_nessesary_tf_elements(edges_chunk, ranks)\n",
    "D_squared = (D - 1) * (D - 1)\n",
    "\n",
    "M = tf.reduce_sum(A)\n",
    "E = tf.exp(-beta / 2 * D_squared)\n",
    "P = 1 / (1 + tf.exp(-2 * beta * D))\n",
    "predicted_M = tf.reduce_sum(E * P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_density(edges_feed, ranks_feed, beta_feed):\n",
    "    total_M = 0\n",
    "    total_predicted_M = 0\n",
    "    sess.run(edges_iterator.initializer, {all_edges: edges_feed})\n",
    "    while True:\n",
    "        try:\n",
    "            partial_M, partial_predicted_M = sess.run([M, predicted_M], {ranks: ranks_feed, beta: beta_feed})\n",
    "            total_M += partial_M\n",
    "            total_predicted_M += partial_predicted_M\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    return total_M, total_predicted_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_M, total_predicted_M = infer_density(all_edges_feed, ranks_feed, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "Generative model parameters are inferred, so we can try to cross-validate results. High value of metric after cross-validation procedure will be an indicator that we chose $\\alpha, \\beta, c$ parameters properly, and calcucated ranks are statistically significant\n",
    "\n",
    "To start a process, we need to calculate edges number between vertices $E_{ij}$ and their directions $p_{ij}$, and to measure the performance of our model. All calculations we should do over the chunked dataset, each chunk will play a role of test part, the rest of dataset will be the train part. The SpringRank paper describes multigraph accuracy as a measure of model performance, formula goes below:\n",
    "$$\\sigma_a = 1 - \\frac{1}{2M}\\sum_{i,j}|{A_{ij} - (A_{ij} + A_{ji})P_{ij}}|$$\n",
    "Where $M$ - overall number of edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to say that we can deal with multigraph accuracy much faster on sparse graph with skipping empty edges in one and in both directions. We can treat both cases in a way described below:\n",
    "\n",
    "1. If $A_{ij} \\neq 0 $ and $A_{ji} = 0$: $E_{ji}(1 - P_{ji}) + |A_{ji} - E_{ji}P_{ji}|$\n",
    "2. If $A_{ij} = 0$ and $A_{ji} = 0$: $E_{ij}$\n",
    "\n",
    "So the accuracy of predictions for such edges can be described by this formula:\n",
    "$$\\sum E_{ij} - \\frac{\\sum_{(i,j), (j,i) \\in G} E_{ij}}{2} - \\sum_{(i,j) \\in G, (j, i) \\in G} E_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_edges(dataset, ranks, beta, c):\n",
    "    dataset[\"from_rank\"] = ranks.loc[[a for a, _ in dataset.index]][\"rank\"].tolist()\n",
    "    dataset[\"to_rank\"] = ranks.loc[[b for _, b in dataset.index]][\"rank\"].tolist()\n",
    "    dataset[\"direction_probability\"] = (1 / (1 + np.exp(-2 * beta * (dataset[\"from_rank\"] - dataset[\"to_rank\"]))))\n",
    "    dataset[\"number_of_edges\"] = c * np.exp(- beta / 2 * (dataset[\"from_rank\"] - dataset[\"to_rank\"] - 1) ** 2)\n",
    "    return dataset[\"direction_probability\"].tolist(), dataset[\"number_of_edges\"].tolist(), predict_edges_sum(dataset, ranks, beta, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dataset, direction, edges, edges_sum):\n",
    "    accuracy_dataset = dataset.copy()\n",
    "    accuracy_dataset = accuracy_dataset.merge(accuracy_dataset.reset_index(), left_index=True, right_on=[\"to\", \"from\"], how=\"left\", suffixes=('', '_reversed'))\n",
    "    accuracy_dataset[\"not_paired\"] = 0\n",
    "    accuracy_dataset.loc[np.isnan(accuracy_dataset[\"value_reversed\"]), \"not_paired\"] = 1\n",
    "    accuracy_dataset[\"direction\"] = direction\n",
    "    accuracy_dataset[\"edges\"] = edges\n",
    "    accuracy_dataset[\"prediction\"] = accuracy_dataset[\"direction\"] * accuracy_dataset[\"edges\"]\n",
    "    accuracy_dataset[\"error\"] = np.abs(accuracy_dataset[\"value\"] - accuracy_dataset[\"prediction\"])\n",
    "    accuracy_dataset[\"non_paired_error\"] = accuracy_dataset[\"not_paired\"] * accuracy_dataset[\"edges\"] * (1 - accuracy_dataset[\"direction\"])\n",
    "    non_active_edges_sum = edges_sum - (accuracy_dataset[\"not_paired\"] * accuracy_dataset[\"edges\"]).sum() - ((1 - accuracy_dataset[\"not_paired\"]) * accuracy_dataset[\"edges\"]).sum() / 2\n",
    "    return 1 - \\\n",
    "        (accuracy_dataset[\"error\"].sum() + accuracy_dataset[\"non_paired_error\"].sum() + non_active_edges_sum) / \\\n",
    "        (accuracy_dataset[\"value\"].sum() + edges_sum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df[\"from\"] = [\"0x1\", \"0x2\", \"0x3\"]\n",
    "test_df[\"to\"] = [\"0x2\", \"0x1\", \"0x4\"]\n",
    "test_df[\"value\"] = [1, 2, 1]\n",
    "test_df = test_df.set_index([\"from\", \"to\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alphas = [0]\n",
    "# alphas = np.logspace(-2, 2, 5)\n",
    "betas = [4922.40704411323]\n",
    "# betas = [288808]\n",
    "split = ShuffleSplit(n_splits=5)\n",
    "# split = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "train_metrics = {}\n",
    "test_metrics = {}\n",
    "\n",
    "train_index, test_index = next(split.split(transactions_df))\n",
    "# for train_index, test_index in :\n",
    "train = create_dataset(transactions_df.loc[train_index])\n",
    "print(\"Train size: {} samples\".format(train.shape[0]))\n",
    "train_addresses = list([a for a, _ in train.index] + [b for _, b in train.index])\n",
    "test = create_dataset(transactions_df.loc[test_index], addresses=train_addresses)\n",
    "print(\"Test size: {} samples\".format(test.shape[0]))\n",
    "for alpha in alphas:\n",
    "    ranks = find_ranks(train, alpha=alpha)\n",
    "    for beta in betas:\n",
    "        c = infer_density(train, ranks, beta=beta)\n",
    "        train_directions, train_edges, train_sum = predict_edges(train, ranks, beta=beta, c=c)\n",
    "        print(beta * c, train_sum)\n",
    "        test_directions, test_edges, test_sum = predict_edges(test, ranks, beta=beta, c=c)\n",
    "        print(train_sum, test_sum)\n",
    "        train_metrics[(alpha, beta, c)] = train_metrics.get((alpha, beta, c), []) + [accuracy(train, train_directions, train_edges, train_sum)]\n",
    "        test_metrics[(alpha, beta, c)] = test_metrics.get((alpha, beta, c), []) + [accuracy(test, test_directions, test_edges, test_sum)]\n",
    "        print(\"Train accuracy: \", train_metrics[(alpha, beta, c)][-1])\n",
    "        print(\"Test accuracy: \", test_metrics[(alpha, beta, c)][-1])\n",
    "pprint(train_metrics)\n",
    "pprint(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_df = pd.DataFrame().from_dict(train_metrics).mean().to_frame().reset_index().rename(columns={\"level_0\": \"alpha\", \"level_1\": \"beta\", 0: \"accuracy\", \"level_2\": \"c\"})\n",
    "test_metrics_df = pd.DataFrame().from_dict(test_metrics).mean().to_frame().reset_index().rename(columns={\"level_0\": \"alpha\", \"level_1\": \"beta\", 0: \"accuracy\", \"level_2\": \"c\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_metrics_df[\"alpha_log\"] = np.log(test_metrics_df[\"alpha\"])\n",
    "train_metrics_df[\"alpha_log\"] = np.log(train_metrics_df[\"alpha\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cross-validation statistics will be plotted there. Now these cells are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_metrics_df.groupby(\"alpha_log\")[\"accuracy\"].mean(), label=\"train\")\n",
    "plt.plot(test_metrics_df.groupby(\"alpha_log\")[\"accuracy\"].mean(), label=\"test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_metrics_df.groupby(\"beta\")[\"accuracy\"].max(), label=\"train\")\n",
    "plt.plot(test_metrics_df.groupby(\"beta\")[\"accuracy\"].max(), label=\"test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_metrics_df.groupby(\"c\")[\"accuracy\"].max(), label=\"train\")\n",
    "plt.plot(test_metrics_df.groupby(\"c\")[\"accuracy\"].max(), label=\"test\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
